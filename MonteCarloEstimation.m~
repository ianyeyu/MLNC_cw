%n is the number of Trace to sample
%T is transitions
%R is returns
%Initial and Absorbing are for states
%Policy is unbiased policy
%gamma is discount factor
function Q = MonteCarloEstimation(T, R, Initial, Absorbing, Policy, gamma, n)
numOfStates = length(Initial);
Return = cell(numOfStates,2);
ZerosMatrix = zeros(size(Policy));
Policy = ZerosMatrix;

for m = 1:n
	Trace = GetTrace(T,R,Initial,Absorbing,Policy); %randomly obtain one Trace
	TraceSize = size(Trace,1);
	
	%each Trace contains several rewards
	for i = 1:TraceSize-1  %Trace is a m*3 matrix, m is the number of steps
		states(i) = Trace(i,2);
		tmp = 0;
			
		%calculate the Returns for each (state, action) pair
		for j = i:TraceSize
			tmp = tmp + Trace(j,1).*(gamma^(j-1));
		end
		
		%upgrade the Return matrix for each state in episode
		%Return is a cell containing 7*2 numeric array 
		Return{Trace(i,2), Trace(i,3)} = [Return{Trace(i,2), Trace(i,3)}, [Trace(i,2),Trace(i,3),tmp]];
	end
	
	Q = cellfun(@mean,Return); %Q is 7*2 numeric matrix
	[~,index] = max(Q,[],2); %find the max rewards action for each state -> 7*1 matrix
	index = [[1:numOfStates]', index];  %concatenate maximum values with states indices -> 7*2 matrix
	states = unique(states); %find the states in the episode
	index = index(states,:); %delete the un-existing states in the eposide
	
	%use temp as a policy upgrader -- it contains the state that needs to be upgraded and actions that obtain best rewards
	%temp is 7*2 matrix -- [state, action]
	temp = zeros(size(index)); 
	for j = 1:size(index,1)
		temp(index(j,2)) = 1;
	end
	Policy(state,:) = [temp];
	
end
end